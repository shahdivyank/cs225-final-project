{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1eCnXs3JC7d1b5nxaiCiOzE-F4bQt4x-v","timestamp":1741586207357}]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11005628,"sourceType":"datasetVersion","datasetId":6851434}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\npath = \"/kaggle/input/uaskdata\"\nimport shutil\nimport zipfile\n\n# Define paths\ndata_path = \"/kaggle/input/uaskdata\"\nworking_path = \"/kaggle/working/uaskdata\"\n\n#Ensure working directory exists\nos.makedirs(working_path, exist_ok=True)\n\n# Unzip all dataset files into /kaggle/working/\nfor file in os.listdir(data_path):\n    if file.endswith(\".zip\"):\n        zip_file_path = os.path.join(data_path, file)\n        print(f\"Extracting {zip_file_path}...\")\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(working_path)\n\nprint(\"Dataset extracted to:\", working_path)\n","metadata":{"id":"SSFplt9qNsQ5","executionInfo":{"status":"ok","timestamp":1741743760716,"user_tz":420,"elapsed":645,"user":{"displayName":"Akash Devappa","userId":"00020031625220567653"}},"outputId":"6b750e96-133b-408c-8c72-82b94abca06a","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:22.260731Z","iopub.execute_input":"2025-03-12T16:55:22.261014Z","iopub.status.idle":"2025-03-12T16:55:22.274518Z","shell.execute_reply.started":"2025-03-12T16:55:22.260992Z","shell.execute_reply":"2025-03-12T16:55:22.273760Z"}},"outputs":[{"name":"stdout","text":"Dataset extracted to: /kaggle/working/uaskdata\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import os\n\n\n\nimport shutil\n\n# Copy dataset from read-only Kaggle input to working directory\nshutil.copytree(\"/kaggle/input/uaskdata\", \"/kaggle/working/uaskdata\", dirs_exist_ok=True)\n\nprint(\"Dataset copied to working directory!\")\n\n\n# Define dataset path\ndata_path = \"/kaggle/working/uaskdata\"\n\n\n\n# Check the `tweet2000000` directory\ntweet2000000_path = os.path.join(data_path, \"tweet2000000\")\nif os.path.exists(tweet2000000_path):\n    print(\"tweet2000000 exists!\")\n    print(\"Files in tweet2000000:\", os.listdir(tweet2000000_path))\nelse:\n    print(\"❌ tweet2000000 folder is missing!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:33.698210Z","iopub.execute_input":"2025-03-12T16:55:33.698579Z","iopub.status.idle":"2025-03-12T16:55:37.491090Z","shell.execute_reply.started":"2025-03-12T16:55:33.698547Z","shell.execute_reply":"2025-03-12T16:55:37.490061Z"}},"outputs":[{"name":"stdout","text":"Dataset copied to working directory!\ntweet2000000 exists!\nFiles in tweet2000000: ['3.txt', '12.txt', '17.txt', '13.txt', '9.txt', '7.txt', '5.txt', '0.txt', '4.txt', '15.txt', '19.txt', '6.txt', '14.txt', '1.txt', '2.txt', '18.txt', '11.txt', '8.txt', '10.txt', '16.txt']\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"data_path = \"/kaggle/input/uaskdata\"\none_billion = f\"{data_path}/tweet10000000\"\ntwo_million = f\"{data_path}/tweet2000000\"\nfour_million = f\"{data_path}/tweet4000000\"\nsix_million = f\"{data_path}/tweet6000000\"\neight_million = f\"{data_path}/tweet8000000\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:38.605782Z","iopub.execute_input":"2025-03-12T16:55:38.606100Z","iopub.status.idle":"2025-03-12T16:55:38.610196Z","shell.execute_reply.started":"2025-03-12T16:55:38.606071Z","shell.execute_reply":"2025-03-12T16:55:38.609227Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"for folder in os.listdir(data_path):\n    folder_path = os.path.join(data_path, folder)\n    if os.path.isdir(folder_path):\n        print(f\" {folder}: {os.listdir(folder_path)[:5]}\")  # Show first 5 files\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:42.331182Z","iopub.execute_input":"2025-03-12T16:55:42.331514Z","iopub.status.idle":"2025-03-12T16:55:42.339850Z","shell.execute_reply.started":"2025-03-12T16:55:42.331487Z","shell.execute_reply":"2025-03-12T16:55:42.338929Z"}},"outputs":[{"name":"stdout","text":" tweet4000000: ['3.txt', '30.txt', '36.txt', '10.txt', '19.txt']\n tweet6000000: ['40.txt', '44.txt', '3.txt', '30.txt', '36.txt']\n tweet2000000: ['3.txt', '10.txt', '19.txt', '5.txt', '7.txt']\n tweet8000000: ['40.txt', '44.txt', '3.txt', '61.txt', '69.txt']\n tweet10000000: ['40.txt', '44.txt', '3.txt', '84.txt', '61.txt']\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import os\nimport pickle\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n### Object Class Definitions ###\nclass ObjectPass1:\n    \"\"\"Represents a spatial object for Pass 1 (only ID and coordinates).\"\"\"\n    def __init__(self, oid, x, y):\n        self.id = oid\n        self.x = x\n        self.y = y\n\nclass Rectangle:\n    \"\"\"Defines a bounding box for Quadtree nodes.\"\"\"\n    def __init__(self, x_min, y_min, x_max, y_max):\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max\n\n    def contains(self, x, y):\n        \"\"\"Check if (x, y) is inside the rectangle (handles precision issues).\"\"\"\n        epsilon = 1e-6  # Small tolerance to prevent floating-point issues\n        return (self.x_min - epsilon <= x <= self.x_max + epsilon) and (self.y_min - epsilon <= y <= self.y_max + epsilon)\n\nclass QuadTree:\n    \"\"\"Quadtree Node Structure.\"\"\"\n    def __init__(self, boundary, capacity=4):\n        self.boundary = boundary  # The rectangular area covered by this node\n        self.capacity = capacity  # Max objects before subdivision\n        self.objects = []  # Objects stored in this node\n        self.children = [None, None, None, None]  # Four quadrants\n        self.is_leaf = True  # Whether this is a leaf node\n        self.neighbors = []  # Store neighboring leaf nodes\n\n### Quadtree Construction ###\ndef build_quad_tree(node, objects, depth=0, max_depth=20):\n    \"\"\"Recursively builds a Quadtree with improved debugging and verification.\"\"\"\n    node.objects = objects  # Store objects in node before splitting\n\n    # Base Case 1: Stop if max depth reached or too few objects\n    if depth >= max_depth or len(objects) <= node.capacity:\n        node.is_leaf = True\n        return\n\n    # Base Case 2: Stop if region size is too small\n    x_min, y_min, x_max, y_max = node.boundary.x_min, node.boundary.y_min, node.boundary.x_max, node.boundary.y_max\n    if abs(x_max - x_min) < 1e-6 or abs(y_max - y_min) < 1e-6:\n        node.is_leaf = True\n        return\n\n    # Create Quadrants\n    x_mid, y_mid = (x_min + x_max) / 2, (y_min + y_max) / 2\n    quadrants = [\n        Rectangle(x_min, y_mid, x_mid, y_max),  # Top-left\n        Rectangle(x_mid, y_mid, x_max, y_max),  # Top-right\n        Rectangle(x_min, y_min, x_mid, y_mid),  # Bottom-left\n        Rectangle(x_mid, y_min, x_max, y_mid)   # Bottom-right\n    ]\n\n    node.children = [QuadTree(q, node.capacity) for q in quadrants]\n\n    # Assign Objects to Children\n    child_objects = {i: [] for i in range(4)}\n    unassigned_objects = []  # Track objects that do not fit\n\n    for obj in objects:\n        assigned = False\n        for i, child in enumerate(node.children):\n            if child.boundary.contains(obj.x, obj.y):\n                child_objects[i].append(obj)\n                assigned = True\n                break\n        if not assigned:\n            unassigned_objects.append(obj)\n\n    # Handle Unassigned Objects\n    if unassigned_objects:\n        node.objects = unassigned_objects  # Keep unassigned objects in this node\n        node.is_leaf = True\n        return\n\n    # Recursively Build Children\n    for i, child in enumerate(node.children):\n        if child_objects[i]: \n            build_quad_tree(child, child_objects[i], depth + 1, max_depth)\n\n    node.objects = []\n\n### Leaf Collection & Neighbor Assignment ###\ndef collect_leaf_nodes(node, leaf_list=None):\n    \"\"\"Recursively collects all leaf nodes in the Quadtree.\"\"\"\n    if leaf_list is None:\n        leaf_list = []\n    if node.is_leaf:\n        leaf_list.append(node)\n    else:\n        for child in node.children:\n            if child is not None:\n                collect_leaf_nodes(child, leaf_list)\n    return leaf_list\n\ndef find_neighbors_fixed(leaf_nodes):\n    \"\"\"Assigns neighbors to each leaf node based on shared boundaries.\"\"\"\n    for node in leaf_nodes:\n        node.neighbors = []\n        for other in leaf_nodes:\n            if node == other:\n                continue\n            shared_x = (node.boundary.x_max == other.boundary.x_min) or (node.boundary.x_min == other.boundary.x_max)\n            shared_y = (node.boundary.y_max == other.boundary.y_min) or (node.boundary.y_min == other.boundary.y_max)\n            if shared_x or shared_y:\n                node.neighbors.append(other)\n\n### Save Location Table ###\ndef save_location_table(root, base_path, batch_size=50):\n    \"\"\"Saves the location table for each leaf node using batch processing.\"\"\"\n    leaf_nodes = collect_leaf_nodes(root)\n    for i in tqdm(range(0, len(leaf_nodes), batch_size), desc=\"Saving Location Tables\", unit=\"batch\"):\n        batch = leaf_nodes[i:i+batch_size]\n        for node in batch:\n            if node.is_leaf:\n                location_table = {obj.id: (obj.x, obj.y) for obj in node.objects}\n                filename = os.path.join(base_path, f\"location_table_{id(node)}.pkl\")\n                with open(filename, \"wb\") as f:\n                    pickle.dump(location_table, f)\n                node.ltp = filename  \n\n### Define Dataset Path & Load Data ###\nbase_path = \"/kaggle/working/uaskdata\"\nos.makedirs(base_path, exist_ok=True)\n\nfilename = os.path.join(base_path, \"tweet2000000\", \"0.txt\")\n\nobjects_pass1 = []\ntry:\n    with open(filename, \"r\") as file:\n        for line in file:\n            parts = line.strip().split()\n            if len(parts) < 4:\n                continue\n            oid, x, y = parts[0], float(parts[1]), float(parts[2])\n            objects_pass1.append(ObjectPass1(oid, x, y))\nexcept FileNotFoundError:\n    print(f\"❌ Error: File not found: {filename}\")\n    exit(1)\n\n### Create Quadtree ###\nmin_x, max_x = min(obj.x for obj in objects_pass1), max(obj.x for obj in objects_pass1)\nmin_y, max_y = min(obj.y for obj in objects_pass1), max(obj.y for obj in objects_pass1)\nroot_boundary = Rectangle(min_x, min_y, max_x, max_y)\nroot = QuadTree(root_boundary, capacity=4)\n\n### Build Quadtree ###\nbuild_quad_tree(root, objects_pass1, depth=0, max_depth=25)\n\n### Assign Neighbors ###\nleaf_nodes = collect_leaf_nodes(root)\nfind_neighbors_fixed(leaf_nodes)\n\n### Save Location Tables ###\nsave_location_table(root, base_path, batch_size=50)\n\n### Save Quadtree ###\nwith open(os.path.join(base_path, \"quadtree.pkl\"), \"wb\") as f:\n    pickle.dump(root, f)\n\n### Final Debugging Check ###\nleaf_nodes = collect_leaf_nodes(root)\nprint(f\"✅ Pass 1 complete: Quadtree built.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:43.398058Z","iopub.execute_input":"2025-03-12T16:55:43.398361Z","iopub.status.idle":"2025-03-12T16:55:50.303438Z","shell.execute_reply.started":"2025-03-12T16:55:43.398336Z","shell.execute_reply":"2025-03-12T16:55:50.302469Z"}},"outputs":[{"name":"stderr","text":"Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 1389.30batch/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Pass 1 complete: Quadtree built.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import os\nimport gzip\nimport json\nimport pickle\nimport hashlib\nfrom tqdm import tqdm\n\nclass ObjectPass2:\n    def __init__(self, obj_id, x, y, keywords, weights):\n        self.id = obj_id\n        self.x = x\n        self.y = y\n        self.keywords = keywords\n        self.weights = weights\n\n# Hash Function for Consistent Node Identification\ndef get_node_hash(node):\n\n    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n\n# Locate the Correct Leaf Node\ndef locate_leaf(root, x, y):\n\n    node = root\n    while not node.is_leaf:\n        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n\n        if y >= y_mid:\n            if x <= x_mid:\n                node = node.children[0]  # Top Left\n            else:\n                node = node.children[1]  # Top Right\n        else:\n            if x <= x_mid:\n                node = node.children[2]  # Bottom Left\n            else:\n                node = node.children[3]  # Bottom Right\n    return node\n\n# Build and Save Textual Indexes Using JSON\ndef build_textual_indexes_as_json(root, base_path):\n    leaf_nodes = collect_leaf_nodes(root)\n    json_data = {}\n\n    print(f\"✅ Processing {len(leaf_nodes)} leaf nodes for textual index...\")\n\n    for node in tqdm(leaf_nodes, desc=\"Building Textual Index\", unit=\"node\"):\n        node_hash = get_node_hash(node)\n\n        # If node is empty, ensure it is indexed with empty structures\n        if not node.objects:\n            json_data[node_hash] = {\"oti\": {}, \"iti\": {}}\n            node.iti = os.path.join(base_path, \"textual_index.json.gz\")\n            continue\n\n        # 1) Build Object Text Index (OTI)\n        text_data = {obj.id: list(zip(obj.keywords, obj.weights)) for obj in node.objects}\n\n        # 2) Build Inverted Textual Index (ITI)\n        inverted_dict = {}\n        for obj in node.objects:\n            for kw, w_str in zip(obj.keywords, obj.weights):\n                w_val = float(w_str)\n                if kw not in inverted_dict:\n                    inverted_dict[kw] = []\n                inverted_dict[kw].append((obj.id, w_val))\n\n        json_data[node_hash] = {\"oti\": text_data, \"iti\": inverted_dict}\n        node.iti = os.path.join(base_path, \"textual_index.json.gz\")\n\n    #Save entire JSON at once with Gzip compression\n    json_filename = os.path.join(base_path, \"textual_index.json.gz\")\n    with gzip.open(json_filename, \"wt\", encoding=\"utf-8\") as f:\n        json.dump(json_data, f)\n\n    print(f\"✅ Saved all textual indexes in: {json_filename} (Compressed)\")\n\n# Define Dataset Path\nbase_path = \"/kaggle/working/uaskdata\"\n\n# Load Quadtree\nquadtree_path = os.path.join(base_path, \"quadtree.pkl\")\nif not os.path.exists(quadtree_path):\n    raise FileNotFoundError(f\"❌ Error: Quadtree file not found at {quadtree_path}. Run Pass 1 first.\")\n\nwith open(quadtree_path, \"rb\") as f:\n    root = pickle.load(f)\nprint(\"✅ Quadtree loaded.\")\n\n#Step 1: Reset objects in all leaves before inserting new ones\nfor leaf in collect_leaf_nodes(root):\n    leaf.objects = []  # Clear previous objects\n\n# Step 2: Read and Process the Dataset Again (Pass 2)\nfilename = os.path.join(base_path, \"tweet2000000\", \"0.txt\")\n\nobjects_loaded = 0  # Debug counter\n\nwith open(filename, \"r\") as file:\n    for line in file:\n        parts = line.strip().split()\n        if len(parts) < 4:\n            print(f\"Skipping invalid line: {line.strip()}\")\n            continue\n\n        oid = parts[0]\n        try:\n            x = float(parts[1])\n            y = float(parts[2])\n            keyword_count = int(parts[3])\n        except ValueError:\n            print(f\"Error: Invalid number format in line: {line.strip()}\")\n            continue\n\n        #Read (keyword, weight) pairs\n        keywords = []\n        weights = []\n        idx = 4\n        for _ in range(keyword_count):\n            try:\n                kw = parts[idx]\n                wt = float(parts[idx + 1])  # Convert weight to float\n                keywords.append(kw)\n                weights.append(wt)\n                idx += 2\n            except (IndexError, ValueError):\n                print(f\"Skipping malformed keyword-weight pair in line: {line.strip()}\")\n                break\n\n        # Create an Object for Pass 2\n        obj2 = ObjectPass2(oid, x, y, keywords, weights)\n\n        # Insert into the correct leaf node\n        leaf_node = locate_leaf(root, x, y)\n        leaf_node.objects.append(obj2)\n        objects_loaded += 1  # Increment debug counter\n\nprint(f\"✅ Loaded {objects_loaded} objects into Quadtree for Pass 2.\")\n\n#Step 3: Build textual indexes using JSON storage\nbuild_textual_indexes_as_json(root, base_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:50.304692Z","iopub.execute_input":"2025-03-12T16:55:50.304957Z","iopub.status.idle":"2025-03-12T16:55:58.767518Z","shell.execute_reply.started":"2025-03-12T16:55:50.304933Z","shell.execute_reply":"2025-03-12T16:55:58.766535Z"}},"outputs":[{"name":"stdout","text":"✅ Quadtree loaded.\n✅ Loaded 100000 objects into Quadtree for Pass 2.\n✅ Processing 1 leaf nodes for textual index...\n","output_type":"stream"},{"name":"stderr","text":"Building Textual Index: 100%|██████████| 1/1 [00:00<00:00,  2.74node/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Saved all textual indexes in: /kaggle/working/uaskdata/textual_index.json.gz (Compressed)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Load the textual index\ntextual_index = load_textual_indexes(\"/kaggle/working/uaskdata\")\n\n# Print a few stored node hashes\nstored_node_hashes = list(textual_index.keys())[:10]  # Print first 10 hashes\nprint(\"✅ Example Node Hashes stored in textual index:\", stored_node_hashes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:55:58.769102Z","iopub.execute_input":"2025-03-12T16:55:58.769383Z","iopub.status.idle":"2025-03-12T16:55:59.199569Z","shell.execute_reply.started":"2025-03-12T16:55:58.769344Z","shell.execute_reply":"2025-03-12T16:55:59.198486Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded textual index from: /kaggle/working/uaskdata/textual_index.json.gz\n✅ Example Node Hashes stored in textual index: ['f78da8ae8ca17ef8']\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import os\nimport json\nimport gzip\nimport heapq\nimport math\nimport pickle\nimport hashlib\nfrom collections import defaultdict\n\n# Query Class\nclass Query:\n    \"\"\"\n    Represents a spatial-textual query.\n    \"\"\"\n    def __init__(self, loc, pos_keywords=None, neg_phrases=None, k=10, lam=0.5):\n        self.loc = loc\n        self.pos = pos_keywords if pos_keywords else []\n        self.neg = neg_phrases if neg_phrases else []\n        self.k = k\n        self.lam = lam  # Lambda controls weighting between spatial & textual scores\n\n# Hash Function for Node Identification\ndef get_node_hash(node):\n    \"\"\"Generates a unique, persistent identifier for a Quadtree node based on its boundary.\"\"\"\n    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n\n# Distance Calculation\ndef euclidean_distance(a, b):\n    \"\"\"Computes Euclidean distance between two points.\"\"\"\n    return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\ndef normalized_spatial_score(obj_loc, query_loc, max_dist=1000.0):\n    \"\"\"Computes a normalized spatial score (1 = closest, 0 = farthest).\"\"\"\n    dist = euclidean_distance(obj_loc, query_loc)\n    return 1.0 - min(dist / max_dist, 1.0)\n\n# Load Textual Index\ndef load_textual_indexes(base_path):\n    \"\"\"Loads the JSON-based textual index from a compressed file.\"\"\"\n    json_filename = os.path.join(base_path, \"textual_index.json.gz\")\n\n    if not os.path.exists(json_filename):\n        raise FileNotFoundError(f\"❌ Error: Textual index file not found at {json_filename}. Run Pass 2 to generate this file.\")\n\n    with gzip.open(json_filename, \"rt\", encoding=\"utf-8\") as f:\n        textual_index = json.load(f)\n\n    print(f\"✅ Loaded textual index from: {json_filename}\")\n    return textual_index\n\n# Negative Keyword Filtering\ndef violates_negative_phrases(obj_id, iti, neg_phrases):\n    \"\"\"\n    Checks if the object contains any negative phrase match.\n    Fix: Uses phrase-based filtering instead of individual word filtering.\n    \"\"\"\n    if not neg_phrases:\n        return False\n\n    for phrase in neg_phrases:\n        words = phrase.split()\n        if all(word in iti and obj_id in [x[0] for x in iti[word]] for word in words):\n            return True\n    return False\n\n# Local POWER Algorithm\ndef local_POWER(node, q, textual_index):\n    \"\"\"\n    Performs local POWER query processing on a given node.\n    Uses the stored textual indexes to rank objects.\n    \"\"\"\n    node_hash = get_node_hash(node)\n\n    if node_hash not in textual_index:\n        return []\n\n    oti = textual_index[node_hash][\"oti\"]\n    iti = textual_index[node_hash][\"iti\"]\n\n    # Step 1: Build a spatially sorted list\n    spatial_list = []\n    for obj_id in oti.keys():\n        obj_loc = node.boundary.x_min, node.boundary.y_min  # Approximate object location\n        sp_score = normalized_spatial_score(obj_loc, q.loc)\n        spatial_list.append((obj_id, sp_score))\n\n    spatial_list.sort(key=lambda x: x[1], reverse=True)\n\n    # Step 2: Textual scoring\n    def compute_textual_score(obj_id):\n        score = 0.0\n        for kw in q.pos:\n            if kw in iti and obj_id in [x[0] for x in iti[kw]]:\n                score += 1.0\n        return min(score, 1.0)\n\n    # Step 3: Compute final ranking\n    local_top_k = []\n    for obj_id, sp_score in spatial_list:\n        txt_score = compute_textual_score(obj_id)\n        if violates_negative_phrases(obj_id, iti, q.neg):\n            continue\n        final_score = q.lam * sp_score + (1 - q.lam) * txt_score\n        heapq.heappush(local_top_k, (-final_score, obj_id))\n\n    return heapq.nsmallest(q.k, local_top_k)\n\n# Locate the Correct Leaf Node\ndef locate_leaf(root, x, y):\n    \"\"\"Finds the correct leaf node where an object (x, y) belongs.\"\"\"\n    node = root\n    while not node.is_leaf:\n        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n\n        if y >= y_mid:\n            if x <= x_mid:\n                node = node.children[0]\n            else:\n                node = node.children[1]\n        else:\n            if x <= x_mid:\n                node = node.children[2]\n            else:\n                node = node.children[3]\n    return node\n\n# **Batch Query Processing**\ndef batch_process_power(root, queries, textual_index):\n    \"\"\"\n    Implements batch query processing for spatial-textual queries.\n    Optimized for shared node access.\n    \"\"\"\n    query_groups = defaultdict(list)\n\n    # Step 1: Group queries by leaf node\n    for query in queries:\n        leaf = locate_leaf(root, query.loc[0], query.loc[1])\n        query_groups[leaf].append(query)\n\n    batch_results = {}\n\n    # Step 2: Process queries in batches\n    for leaf, grouped_queries in query_groups.items():\n        node_hash = get_node_hash(leaf)\n\n        if node_hash not in textual_index:\n            continue\n\n        results_per_query = {}\n        for q in grouped_queries:\n            results_per_query[q] = local_POWER(leaf, q, textual_index[node_hash])\n\n        batch_results[node_hash] = results_per_query\n\n    return batch_results\n\n# Load Quadtree and Textual Index\nbase_path = \"/kaggle/working/uaskdata\"\n\nwith open(os.path.join(base_path, \"quadtree.pkl\"), \"rb\") as f:\n    root = pickle.load(f)\nprint(\"Quadtree loaded.\")\n\ntextual_index = load_textual_indexes(base_path)\n\n# Define Sample Queries\nqueries = [\n    Query(loc=(500.0, 500.0), pos_keywords=[\"pizza\", \"chicago\"], neg_phrases=[\"not good\"], k=5, lam=0.4),\n    Query(loc=(700.0, 300.0), pos_keywords=[\"burger\"], neg_phrases=[], k=3, lam=0.5),\n    Query(loc=(200.0, 800.0), pos_keywords=[\"sushi\"], neg_phrases=[\"bad\"], k=7, lam=0.6),\n]\n\n# Run Batch Query Processing\nbatch_results = batch_process_power(root, queries, textual_index)\n\n# Display Results\nfor node_hash, query_results in batch_results.items():\n    print(f\"Node Hash: {node_hash}\")\n    for query, results in query_results.items():\n        print(f\" Query at {query.loc} => {results}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T17:09:34.104464Z","iopub.execute_input":"2025-03-12T17:09:34.104767Z","iopub.status.idle":"2025-03-12T17:09:38.783386Z","shell.execute_reply.started":"2025-03-12T17:09:34.104744Z","shell.execute_reply":"2025-03-12T17:09:38.782469Z"}},"outputs":[{"name":"stdout","text":"Quadtree loaded.\n✅ Loaded textual index from: /kaggle/working/uaskdata/textual_index.json.gz\nNode Hash: f78da8ae8ca17ef8\n Query at (500.0, 500.0) => []\n Query at (700.0, 300.0) => []\n Query at (200.0, 800.0) => []\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}