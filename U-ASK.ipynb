{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:22.261014Z",
     "iopub.status.busy": "2025-03-12T16:55:22.260731Z",
     "iopub.status.idle": "2025-03-12T16:55:22.274518Z",
     "shell.execute_reply": "2025-03-12T16:55:22.273760Z",
     "shell.execute_reply.started": "2025-03-12T16:55:22.260992Z"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1741743760716,
     "user": {
      "displayName": "Akash Devappa",
      "userId": "00020031625220567653"
     },
     "user_tz": 420
    },
    "id": "SSFplt9qNsQ5",
    "outputId": "6b750e96-133b-408c-8c72-82b94abca06a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to: /kaggle/working/uaskdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"/kaggle/input/uaskdata\"\n",
    "import zipfile\n",
    "\n",
    "# Define paths\n",
    "data_path = \"/kaggle/input/uaskdata\"\n",
    "working_path = \"/kaggle/working/uaskdata\"\n",
    "\n",
    "#Ensure working directory exists\n",
    "os.makedirs(working_path, exist_ok=True)\n",
    "\n",
    "# Unzip all dataset files into /kaggle/working/\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zip_file_path = os.path.join(data_path, file)\n",
    "        print(f\"Extracting {zip_file_path}...\")\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(working_path)\n",
    "\n",
    "print(\"Dataset extracted to:\", working_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:33.698579Z",
     "iopub.status.busy": "2025-03-12T16:55:33.698210Z",
     "iopub.status.idle": "2025-03-12T16:55:37.491090Z",
     "shell.execute_reply": "2025-03-12T16:55:37.490061Z",
     "shell.execute_reply.started": "2025-03-12T16:55:33.698547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to working directory!\n",
      "tweet2000000 exists!\n",
      "Files in tweet2000000: ['3.txt', '12.txt', '17.txt', '13.txt', '9.txt', '7.txt', '5.txt', '0.txt', '4.txt', '15.txt', '19.txt', '6.txt', '14.txt', '1.txt', '2.txt', '18.txt', '11.txt', '8.txt', '10.txt', '16.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Copy dataset from read-only Kaggle input to working directory\n",
    "shutil.copytree(\"/kaggle/input/uaskdata\", \"/kaggle/working/uaskdata\", dirs_exist_ok=True)\n",
    "\n",
    "print(\"Dataset copied to working directory!\")\n",
    "\n",
    "\n",
    "# Define dataset path\n",
    "data_path = \"/kaggle/working/uaskdata\"\n",
    "\n",
    "\n",
    "\n",
    "# Check the `tweet2000000` directory\n",
    "tweet2000000_path = os.path.join(data_path, \"tweet2000000\")\n",
    "if os.path.exists(tweet2000000_path):\n",
    "    print(\"tweet2000000 exists!\")\n",
    "    print(\"Files in tweet2000000:\", os.listdir(tweet2000000_path))\n",
    "else:\n",
    "    print(\"❌ tweet2000000 folder is missing!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:38.606100Z",
     "iopub.status.busy": "2025-03-12T16:55:38.605782Z",
     "iopub.status.idle": "2025-03-12T16:55:38.610196Z",
     "shell.execute_reply": "2025-03-12T16:55:38.609227Z",
     "shell.execute_reply.started": "2025-03-12T16:55:38.606071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/uaskdata\"\n",
    "one_billion = f\"{data_path}/tweet10000000\"\n",
    "two_million = f\"{data_path}/tweet2000000\"\n",
    "four_million = f\"{data_path}/tweet4000000\"\n",
    "six_million = f\"{data_path}/tweet6000000\"\n",
    "eight_million = f\"{data_path}/tweet8000000\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:42.331514Z",
     "iopub.status.busy": "2025-03-12T16:55:42.331182Z",
     "iopub.status.idle": "2025-03-12T16:55:42.339850Z",
     "shell.execute_reply": "2025-03-12T16:55:42.338929Z",
     "shell.execute_reply.started": "2025-03-12T16:55:42.331487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tweet4000000: ['3.txt', '30.txt', '36.txt', '10.txt', '19.txt']\n",
      " tweet6000000: ['40.txt', '44.txt', '3.txt', '30.txt', '36.txt']\n",
      " tweet2000000: ['3.txt', '10.txt', '19.txt', '5.txt', '7.txt']\n",
      " tweet8000000: ['40.txt', '44.txt', '3.txt', '61.txt', '69.txt']\n",
      " tweet10000000: ['40.txt', '44.txt', '3.txt', '84.txt', '61.txt']\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\" {folder}: {os.listdir(folder_path)[:5]}\")  # Show first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:43.398361Z",
     "iopub.status.busy": "2025-03-12T16:55:43.398058Z",
     "iopub.status.idle": "2025-03-12T16:55:50.303438Z",
     "shell.execute_reply": "2025-03-12T16:55:50.302469Z",
     "shell.execute_reply.started": "2025-03-12T16:55:43.398336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 1389.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pass 1 complete: Quadtree built.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Object Class Definitions ###\n",
    "class ObjectPass1:\n",
    "    \"\"\"Represents a spatial object for Pass 1 (only ID and coordinates).\"\"\"\n",
    "    def __init__(self, oid, x, y):\n",
    "        self.id = oid\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class Rectangle:\n",
    "    \"\"\"Defines a bounding box for Quadtree nodes.\"\"\"\n",
    "    def __init__(self, x_min, y_min, x_max, y_max):\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "\n",
    "    def contains(self, x, y):\n",
    "        \"\"\"Check if (x, y) is inside the rectangle (handles precision issues).\"\"\"\n",
    "        epsilon = 1e-6  # Small tolerance to prevent floating-point issues\n",
    "        return (self.x_min - epsilon <= x <= self.x_max + epsilon) and (self.y_min - epsilon <= y <= self.y_max + epsilon)\n",
    "\n",
    "class QuadTree:\n",
    "    \"\"\"Quadtree Node Structure.\"\"\"\n",
    "    def __init__(self, boundary, capacity=4):\n",
    "        self.boundary = boundary  # The rectangular area covered by this node\n",
    "        self.capacity = capacity  # Max objects before subdivision\n",
    "        self.objects = []  # Objects stored in this node\n",
    "        self.children = [None, None, None, None]  # Four quadrants\n",
    "        self.is_leaf = True  # Whether this is a leaf node\n",
    "        self.neighbors = []  # Store neighboring leaf nodes\n",
    "\n",
    "### Quadtree Construction ###\n",
    "def build_quad_tree(node, objects, depth=0, max_depth=20):\n",
    "    \"\"\"Recursively builds a Quadtree with improved debugging and verification.\"\"\"\n",
    "    node.objects = objects  # Store objects in node before splitting\n",
    "\n",
    "    # Base Case 1: Stop if max depth reached or too few objects\n",
    "    if depth >= max_depth or len(objects) <= node.capacity:\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Base Case 2: Stop if region size is too small\n",
    "    x_min, y_min, x_max, y_max = node.boundary.x_min, node.boundary.y_min, node.boundary.x_max, node.boundary.y_max\n",
    "    if abs(x_max - x_min) < 1e-6 or abs(y_max - y_min) < 1e-6:\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Create Quadrants\n",
    "    x_mid, y_mid = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "    quadrants = [\n",
    "        Rectangle(x_min, y_mid, x_mid, y_max),  # Top-left\n",
    "        Rectangle(x_mid, y_mid, x_max, y_max),  # Top-right\n",
    "        Rectangle(x_min, y_min, x_mid, y_mid),  # Bottom-left\n",
    "        Rectangle(x_mid, y_min, x_max, y_mid)   # Bottom-right\n",
    "    ]\n",
    "\n",
    "    node.children = [QuadTree(q, node.capacity) for q in quadrants]\n",
    "\n",
    "    # Assign Objects to Children\n",
    "    child_objects = {i: [] for i in range(4)}\n",
    "    unassigned_objects = []  # Track objects that do not fit\n",
    "\n",
    "    for obj in objects:\n",
    "        assigned = False\n",
    "        for i, child in enumerate(node.children):\n",
    "            if child.boundary.contains(obj.x, obj.y):\n",
    "                child_objects[i].append(obj)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            unassigned_objects.append(obj)\n",
    "\n",
    "    # Handle Unassigned Objects\n",
    "    if unassigned_objects:\n",
    "        node.objects = unassigned_objects  # Keep unassigned objects in this node\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Recursively Build Children\n",
    "    for i, child in enumerate(node.children):\n",
    "        if child_objects[i]: \n",
    "            build_quad_tree(child, child_objects[i], depth + 1, max_depth)\n",
    "\n",
    "    node.objects = []\n",
    "\n",
    "### Leaf Collection & Neighbor Assignment ###\n",
    "def collect_leaf_nodes(node, leaf_list=None):\n",
    "    \"\"\"Recursively collects all leaf nodes in the Quadtree.\"\"\"\n",
    "    if leaf_list is None:\n",
    "        leaf_list = []\n",
    "    if node.is_leaf:\n",
    "        leaf_list.append(node)\n",
    "    else:\n",
    "        for child in node.children:\n",
    "            if child is not None:\n",
    "                collect_leaf_nodes(child, leaf_list)\n",
    "    return leaf_list\n",
    "\n",
    "def find_neighbors_fixed(leaf_nodes):\n",
    "    \"\"\"Assigns neighbors to each leaf node based on shared boundaries.\"\"\"\n",
    "    for node in leaf_nodes:\n",
    "        node.neighbors = []\n",
    "        for other in leaf_nodes:\n",
    "            if node == other:\n",
    "                continue\n",
    "            shared_x = (node.boundary.x_max == other.boundary.x_min) or (node.boundary.x_min == other.boundary.x_max)\n",
    "            shared_y = (node.boundary.y_max == other.boundary.y_min) or (node.boundary.y_min == other.boundary.y_max)\n",
    "            if shared_x or shared_y:\n",
    "                node.neighbors.append(other)\n",
    "\n",
    "### Save Location Table ###\n",
    "def save_location_table(root, base_path, batch_size=50):\n",
    "    \"\"\"Saves the location table for each leaf node using batch processing.\"\"\"\n",
    "    leaf_nodes = collect_leaf_nodes(root)\n",
    "    for i in tqdm(range(0, len(leaf_nodes), batch_size), desc=\"Saving Location Tables\", unit=\"batch\"):\n",
    "        batch = leaf_nodes[i:i+batch_size]\n",
    "        for node in batch:\n",
    "            if node.is_leaf:\n",
    "                location_table = {obj.id: (obj.x, obj.y) for obj in node.objects}\n",
    "                filename = os.path.join(base_path, f\"location_table_{id(node)}.pkl\")\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    pickle.dump(location_table, f)\n",
    "                node.ltp = filename  \n",
    "\n",
    "### Define Dataset Path & Load Data ###\n",
    "base_path = \"/kaggle/working/uaskdata\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "filename = os.path.join(base_path, \"tweet2000000\", \"0.txt\")\n",
    "\n",
    "objects_pass1 = []\n",
    "try:\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            oid, x, y = parts[0], float(parts[1]), float(parts[2])\n",
    "            objects_pass1.append(ObjectPass1(oid, x, y))\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: File not found: {filename}\")\n",
    "    exit(1)\n",
    "\n",
    "### Create Quadtree ###\n",
    "min_x, max_x = min(obj.x for obj in objects_pass1), max(obj.x for obj in objects_pass1)\n",
    "min_y, max_y = min(obj.y for obj in objects_pass1), max(obj.y for obj in objects_pass1)\n",
    "root_boundary = Rectangle(min_x, min_y, max_x, max_y)\n",
    "root = QuadTree(root_boundary, capacity=4)\n",
    "\n",
    "### Build Quadtree ###\n",
    "build_quad_tree(root, objects_pass1, depth=0, max_depth=25)\n",
    "\n",
    "### Assign Neighbors ###\n",
    "leaf_nodes = collect_leaf_nodes(root)\n",
    "find_neighbors_fixed(leaf_nodes)\n",
    "\n",
    "### Save Location Tables ###\n",
    "save_location_table(root, base_path, batch_size=50)\n",
    "\n",
    "### Save Quadtree ###\n",
    "with open(os.path.join(base_path, \"quadtree.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(root, f)\n",
    "\n",
    "### Final Debugging Check ###\n",
    "leaf_nodes = collect_leaf_nodes(root)\n",
    "print(f\"✅ Pass 1 complete: Quadtree built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:50.304957Z",
     "iopub.status.busy": "2025-03-12T16:55:50.304692Z",
     "iopub.status.idle": "2025-03-12T16:55:58.767518Z",
     "shell.execute_reply": "2025-03-12T16:55:58.766535Z",
     "shell.execute_reply.started": "2025-03-12T16:55:50.304933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quadtree loaded.\n",
      "✅ Loaded 100000 objects into Quadtree for Pass 2.\n",
      "✅ Processing 1 leaf nodes for textual index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Textual Index: 100%|██████████| 1/1 [00:00<00:00,  2.74node/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all textual indexes in: /kaggle/working/uaskdata/textual_index.json.gz (Compressed)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ObjectPass2:\n",
    "    def __init__(self, obj_id, x, y, keywords, weights):\n",
    "        self.id = obj_id\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.keywords = keywords\n",
    "        self.weights = weights\n",
    "\n",
    "# Hash Function for Consistent Node Identification\n",
    "def get_node_hash(node):\n",
    "\n",
    "    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n",
    "\n",
    "# Locate the Correct Leaf Node\n",
    "def locate_leaf(root, x, y):\n",
    "\n",
    "    node = root\n",
    "    while not node.is_leaf:\n",
    "        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n",
    "        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n",
    "\n",
    "        if y >= y_mid:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[0]  # Top Left\n",
    "            else:\n",
    "                node = node.children[1]  # Top Right\n",
    "        else:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[2]  # Bottom Left\n",
    "            else:\n",
    "                node = node.children[3]  # Bottom Right\n",
    "    return node\n",
    "\n",
    "# Build and Save Textual Indexes Using JSON\n",
    "def build_textual_indexes_as_json(root, base_path):\n",
    "    leaf_nodes = collect_leaf_nodes(root)\n",
    "    json_data = {}\n",
    "\n",
    "    print(f\"✅ Processing {len(leaf_nodes)} leaf nodes for textual index...\")\n",
    "\n",
    "    for node in tqdm(leaf_nodes, desc=\"Building Textual Index\", unit=\"node\"):\n",
    "        node_hash = get_node_hash(node)\n",
    "\n",
    "        # If node is empty, ensure it is indexed with empty structures\n",
    "        if not node.objects:\n",
    "            json_data[node_hash] = {\"oti\": {}, \"iti\": {}}\n",
    "            node.iti = os.path.join(base_path, \"textual_index.json.gz\")\n",
    "            continue\n",
    "\n",
    "        # 1) Build Object Text Index (OTI)\n",
    "        text_data = {obj.id: list(zip(obj.keywords, obj.weights)) for obj in node.objects}\n",
    "\n",
    "        # 2) Build Inverted Textual Index (ITI)\n",
    "        inverted_dict = {}\n",
    "        for obj in node.objects:\n",
    "            for kw, w_str in zip(obj.keywords, obj.weights):\n",
    "                w_val = float(w_str)\n",
    "                if kw not in inverted_dict:\n",
    "                    inverted_dict[kw] = []\n",
    "                inverted_dict[kw].append((obj.id, w_val))\n",
    "\n",
    "        json_data[node_hash] = {\"oti\": text_data, \"iti\": inverted_dict}\n",
    "        node.iti = os.path.join(base_path, \"textual_index.json.gz\")\n",
    "\n",
    "    #Save entire JSON at once with Gzip compression\n",
    "    json_filename = os.path.join(base_path, \"textual_index.json.gz\")\n",
    "    with gzip.open(json_filename, \"wt\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f)\n",
    "\n",
    "    print(f\"✅ Saved all textual indexes in: {json_filename} (Compressed)\")\n",
    "\n",
    "# Define Dataset Path\n",
    "base_path = \"/kaggle/working/uaskdata\"\n",
    "\n",
    "# Load Quadtree\n",
    "quadtree_path = os.path.join(base_path, \"quadtree.pkl\")\n",
    "if not os.path.exists(quadtree_path):\n",
    "    raise FileNotFoundError(f\"❌ Error: Quadtree file not found at {quadtree_path}. Run Pass 1 first.\")\n",
    "\n",
    "with open(quadtree_path, \"rb\") as f:\n",
    "    root = pickle.load(f)\n",
    "print(\"✅ Quadtree loaded.\")\n",
    "\n",
    "#Step 1: Reset objects in all leaves before inserting new ones\n",
    "for leaf in collect_leaf_nodes(root):\n",
    "    leaf.objects = []  # Clear previous objects\n",
    "\n",
    "# Step 2: Read and Process the Dataset Again (Pass 2)\n",
    "filename = os.path.join(base_path, \"tweet2000000\", \"0.txt\")\n",
    "\n",
    "objects_loaded = 0  # Debug counter\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 4:\n",
    "            print(f\"Skipping invalid line: {line.strip()}\")\n",
    "            continue\n",
    "\n",
    "        oid = parts[0]\n",
    "        try:\n",
    "            x = float(parts[1])\n",
    "            y = float(parts[2])\n",
    "            keyword_count = int(parts[3])\n",
    "        except ValueError:\n",
    "            print(f\"Error: Invalid number format in line: {line.strip()}\")\n",
    "            continue\n",
    "\n",
    "        #Read (keyword, weight) pairs\n",
    "        keywords = []\n",
    "        weights = []\n",
    "        idx = 4\n",
    "        for _ in range(keyword_count):\n",
    "            try:\n",
    "                kw = parts[idx]\n",
    "                wt = float(parts[idx + 1])  # Convert weight to float\n",
    "                keywords.append(kw)\n",
    "                weights.append(wt)\n",
    "                idx += 2\n",
    "            except (IndexError, ValueError):\n",
    "                print(f\"Skipping malformed keyword-weight pair in line: {line.strip()}\")\n",
    "                break\n",
    "\n",
    "        # Create an Object for Pass 2\n",
    "        obj2 = ObjectPass2(oid, x, y, keywords, weights)\n",
    "\n",
    "        # Insert into the correct leaf node\n",
    "        leaf_node = locate_leaf(root, x, y)\n",
    "        leaf_node.objects.append(obj2)\n",
    "        objects_loaded += 1  # Increment debug counter\n",
    "\n",
    "print(f\"✅ Loaded {objects_loaded} objects into Quadtree for Pass 2.\")\n",
    "\n",
    "#Step 3: Build textual indexes using JSON storage\n",
    "build_textual_indexes_as_json(root, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:58.769383Z",
     "iopub.status.busy": "2025-03-12T16:55:58.769102Z",
     "iopub.status.idle": "2025-03-12T16:55:59.199569Z",
     "shell.execute_reply": "2025-03-12T16:55:59.198486Z",
     "shell.execute_reply.started": "2025-03-12T16:55:58.769344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded textual index from: /kaggle/working/uaskdata/textual_index.json.gz\n",
      "✅ Example Node Hashes stored in textual index: ['f78da8ae8ca17ef8']\n"
     ]
    }
   ],
   "source": [
    "# Load the Textual Index\n",
    "def load_textual_indexes(base_path):\n",
    "    \"\"\"Loads the JSON-based textual index from a compressed file.\"\"\"\n",
    "    json_filename = os.path.join(base_path, \"textual_index.json.gz\")\n",
    "\n",
    "    if not os.path.exists(json_filename):\n",
    "        raise FileNotFoundError(f\"❌ Error: Textual index file not found at {json_filename}. Run Pass 2 to generate this file.\")\n",
    "\n",
    "    with gzip.open(json_filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "        textual_index = json.load(f)\n",
    "\n",
    "    print(f\"✅ Loaded textual index from: {json_filename}\")\n",
    "    return textual_index\n",
    "\n",
    "# Load the textual index\n",
    "textual_index = load_textual_indexes(\"/kaggle/working/uaskdata\")\n",
    "\n",
    "# Print a few stored node hashes\n",
    "stored_node_hashes = list(textual_index.keys())[:10]  # Print first 10 hashes\n",
    "print(\"✅ Example Node Hashes stored in textual index:\", stored_node_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:59.201137Z",
     "iopub.status.busy": "2025-03-12T16:55:59.200789Z",
     "iopub.status.idle": "2025-03-12T16:56:06.134127Z",
     "shell.execute_reply": "2025-03-12T16:56:06.133251Z",
     "shell.execute_reply.started": "2025-03-12T16:55:59.201110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadtree loaded.\n",
      "✅ Loaded textual index from: /kaggle/working/uaskdata/textual_index.json.gz\n",
      "Warning: No index found for Node f78da8ae8ca17ef8\n",
      "Top-k results => []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import heapq\n",
    "import math\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, loc, pos_keywords=None, neg_phrases=None, k=10, lam=0.5):\n",
    "        self.loc = loc\n",
    "        self.pos = pos_keywords if pos_keywords else []\n",
    "        self.neg = neg_phrases if neg_phrases else []\n",
    "        self.k = k\n",
    "        self.lam = lam  # Lambda controls weighting between spatial & textual scores\n",
    "\n",
    "# Hash Function for Node Identification\n",
    "def get_node_hash(node):\n",
    "\n",
    "    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n",
    "\n",
    "# Distance Calculation\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Computes Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n",
    "\n",
    "def normalized_spatial_score(obj_loc, query_loc, max_dist=1000.0):\n",
    "    \"\"\"Computes a normalized spatial score (1 = closest, 0 = farthest).\"\"\"\n",
    "    dist = euclidean_distance(obj_loc, query_loc)\n",
    "    return 1.0 - min(dist / max_dist, 1.0)\n",
    "\n",
    "# Phrase-based Negative Keyword Filtering\n",
    "def violates_negative_phrases(obj_id, iti, neg_phrases):\n",
    "\n",
    "    if not neg_phrases:\n",
    "        return False  # No negative constraints\n",
    "\n",
    "    for phrase in neg_phrases:\n",
    "        words = phrase.split()  # Convert phrase into words\n",
    "        if all(word in iti and obj_id in [x[0] for x in iti[word]] for word in words):\n",
    "            return True  # All words in the phrase are found\n",
    "    return False  # Object does not violate constraints\n",
    "\n",
    "#Local POWER Algorithm\n",
    "def local_POWER(node, q, textual_index):\n",
    "    \"\"\"\n",
    "    Performs local POWER query processing on a given node.\n",
    "    Uses the stored textual indexes to rank objects.\n",
    "    \"\"\"\n",
    "    node_hash = get_node_hash(node)  \n",
    "    if node_hash not in textual_index:\n",
    "        print(f\"Warning: No index found for Node {node_hash}\")\n",
    "        return []\n",
    "\n",
    "    oti = textual_index[node_hash][\"oti\"]\n",
    "    iti = textual_index[node_hash][\"iti\"]\n",
    "\n",
    "    # Step 1: Build a spatially sorted list\n",
    "    spatial_list = []\n",
    "    for obj_id in oti.keys():\n",
    "        obj_loc = node.boundary.x_min, node.boundary.y_min  # Approximate object location\n",
    "        sp_score = normalized_spatial_score(obj_loc, q.loc)\n",
    "        spatial_list.append((obj_id, sp_score))\n",
    "\n",
    "    spatial_list.sort(key=lambda x: x[1], reverse=True)  # Sort by descending sp_score\n",
    "\n",
    "    #  Step 2: Textual scoring (Normalized)\n",
    "    def compute_textual_score(obj_id):\n",
    "        score = 0.0\n",
    "        for kw in q.pos:\n",
    "            if kw in iti and obj_id in [x[0] for x in iti[kw]]:\n",
    "                score += 1.0  # Basic keyword matching score (can be weighted)\n",
    "        return min(score, 1.0)  # Clamp max score to 1.0\n",
    "\n",
    "    # Step 3: Compute final ranking\n",
    "    local_top_k = []\n",
    "    for obj_id, sp_score in spatial_list:\n",
    "        txt_score = compute_textual_score(obj_id)\n",
    "        \n",
    "        # Skip objects violating negative keywords\n",
    "        if violates_negative_phrases(obj_id, iti, q.neg):\n",
    "            continue\n",
    "        \n",
    "        final_score = q.lam * sp_score + (1 - q.lam) * txt_score\n",
    "        heapq.heappush(local_top_k, (-final_score, obj_id))  # Use negative score for max heap\n",
    "\n",
    "    return heapq.nsmallest(q.k, local_top_k)\n",
    "\n",
    "# Locate the Correct Leaf Node\n",
    "def locate_leaf(root, x, y):\n",
    "    \"\"\"\n",
    "    Finds the correct leaf node where an object (x, y) belongs.\n",
    "    \"\"\"\n",
    "    node = root\n",
    "    while not node.is_leaf:\n",
    "        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n",
    "        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n",
    "\n",
    "        if y >= y_mid:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[0]  # Top Left\n",
    "            else:\n",
    "                node = node.children[1]  # Top Right\n",
    "        else:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[2]  # Bottom Left\n",
    "            else:\n",
    "                node = node.children[3]  # Bottom Right\n",
    "    return node\n",
    "\n",
    "# Master TKQN Query Processing\n",
    "def master_process_power(root, q, textual_index):\n",
    "    start_leaf = locate_leaf(root, q.loc[0], q.loc[1])\n",
    "    visited = set()\n",
    "    cell_queue = []\n",
    "    heapq.heappush(cell_queue, (0, start_leaf))\n",
    "\n",
    "    global_top_k = []\n",
    "\n",
    "    while cell_queue:\n",
    "        _, cur_node = heapq.heappop(cell_queue)\n",
    "        if cur_node in visited:\n",
    "            continue\n",
    "        visited.add(cur_node)\n",
    "\n",
    "        node_hash = get_node_hash(cur_node)  \n",
    "\n",
    "        if node_hash not in textual_index:\n",
    "            continue  # Skip this node\n",
    "\n",
    "        local_results = local_POWER(cur_node, q, textual_index[node_hash])\n",
    "        for score, obj_id in local_results:\n",
    "            if len(global_top_k) < q.k:\n",
    "                heapq.heappush(global_top_k, (score, obj_id))\n",
    "            else:\n",
    "                if score > global_top_k[0][0]:\n",
    "                    heapq.heapreplace(global_top_k, (score, obj_id))\n",
    "\n",
    "        # Expand neighboring nodes\n",
    "        for neighbor in cur_node.neighbors:\n",
    "            if neighbor not in visited:\n",
    "                heapq.heappush(cell_queue, (0, neighbor))  \n",
    "\n",
    "    return sorted(global_top_k, key=lambda x: -x[0])\n",
    "\n",
    "# Load Quadtree and Textual Index\n",
    "base_path = \"/kaggle/working/uaskdata\"\n",
    "\n",
    "with open(os.path.join(base_path, \"quadtree.pkl\"), \"rb\") as f:\n",
    "    root = pickle.load(f)\n",
    "print(\"Quadtree loaded.\")\n",
    "\n",
    "textual_index = load_textual_indexes(base_path)\n",
    "\n",
    "# Define a Sample Query\n",
    "my_query = Query(\n",
    "    loc=(500.0, 500.0),\n",
    "    pos_keywords=[\"pizza\", \"chicago\"],\n",
    "    neg_phrases=[\"not good\"],\n",
    "    k=5,\n",
    "    lam=0.4\n",
    ")\n",
    "\n",
    "#Run the Query\n",
    "results = master_process_power(root, my_query, textual_index)\n",
    "\n",
    "#Display Results\n",
    "print(\"Top-k results =>\", results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1eCnXs3JC7d1b5nxaiCiOzE-F4bQt4x-v",
     "timestamp": 1741586207357
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6851434,
     "sourceId": 11005628,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
