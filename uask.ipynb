{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:22.261014Z",
     "iopub.status.busy": "2025-03-12T16:55:22.260731Z",
     "iopub.status.idle": "2025-03-12T16:55:22.274518Z",
     "shell.execute_reply": "2025-03-12T16:55:22.273760Z",
     "shell.execute_reply.started": "2025-03-12T16:55:22.260992Z"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1741743760716,
     "user": {
      "displayName": "Akash Devappa",
      "userId": "00020031625220567653"
     },
     "user_tz": 420
    },
    "id": "SSFplt9qNsQ5",
    "outputId": "6b750e96-133b-408c-8c72-82b94abca06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to: /kaggle/working/uaskdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"/kaggle/input/uaskdata\"\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Define paths\n",
    "data_path = \"/kaggle/input/uaskdata\"\n",
    "working_path = \"/kaggle/working/uaskdata\"\n",
    "\n",
    "#Ensure working directory exists\n",
    "os.makedirs(working_path, exist_ok=True)\n",
    "\n",
    "# Unzip all dataset files into /kaggle/working/\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zip_file_path = os.path.join(data_path, file)\n",
    "        print(f\"Extracting {zip_file_path}...\")\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(working_path)\n",
    "\n",
    "print(\"Dataset extracted to:\", working_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to working directory!\n",
      "tweet2000000 exists!\n",
      "Files in tweet2000000: ['15.txt', '14.txt', '16.txt', '17.txt', '13.txt', '12.txt', '10.txt', '11.txt', '9.txt', '8.txt', '5.txt', '4.txt', '6.txt', '7.txt', '3.txt', '2.txt', '0.txt', '1.txt', '19.txt', '18.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import shutil\n",
    "\n",
    "# Copy dataset from read-only Kaggle input to working directory\n",
    "# shutil.copytree(data_path, working_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Dataset copied to working directory!\")\n",
    "\n",
    "# Define dataset path\n",
    "data_path = \"/Users/shahdivyank/Desktop/cs225-final-project\"\n",
    "\n",
    "# Check the `tweet2000000` directory\n",
    "tweet2000000_path = os.path.join(data_path, \"tweet2000000\")\n",
    "if os.path.exists(tweet2000000_path):\n",
    "    print(\"tweet2000000 exists!\")\n",
    "    print(\"Files in tweet2000000:\", os.listdir(tweet2000000_path))\n",
    "else:\n",
    "    print(\"❌ tweet2000000 folder is missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_million = f\"{data_path}/tweet10000000\"\n",
    "two_million = f\"{data_path}/tweet2000000\"\n",
    "four_million = f\"{data_path}/tweet4000000\"\n",
    "six_million = f\"{data_path}/tweet6000000\"\n",
    "eight_million = f\"{data_path}/tweet8000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tweet2000000: ['15.txt', '14.txt', '16.txt', '17.txt', '13.txt']\n",
      " tweet4000000: ['29.txt', '15.txt', '14.txt', '28.txt', '16.txt']\n",
      " tweet6000000: ['29.txt', '15.txt', '14.txt', '28.txt', '16.txt']\n",
      " tweet10000000: ['29.txt', '15.txt', '14.txt', '28.txt', '16.txt']\n",
      " tweet8000000: ['29.txt', '15.txt', '14.txt', '28.txt', '16.txt']\n",
      " .git: ['config', 'objects', 'HEAD', 'info', 'logs']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\" {folder}: {os.listdir(folder_path)[:5]}\")  # Show first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Object Class Definitions ###\n",
    "class ObjectPass1:\n",
    "    \"\"\"Represents a spatial object for Pass 1 (only ID and coordinates).\"\"\"\n",
    "    def __init__(self, oid, x, y):\n",
    "        self.id = oid\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class Rectangle:\n",
    "    \"\"\"Defines a bounding box for Quadtree nodes.\"\"\"\n",
    "    def __init__(self, x_min, y_min, x_max, y_max):\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "\n",
    "    def contains(self, x, y):\n",
    "        \"\"\"Check if (x, y) is inside the rectangle (handles precision issues).\"\"\"\n",
    "        epsilon = 1e-6  # Small tolerance to prevent floating-point issues\n",
    "        return (self.x_min - epsilon <= x <= self.x_max + epsilon) and (self.y_min - epsilon <= y <= self.y_max + epsilon)\n",
    "\n",
    "class QuadTree:\n",
    "    \"\"\"Quadtree Node Structure.\"\"\"\n",
    "    def __init__(self, boundary, capacity=4):\n",
    "        self.boundary = boundary  # The rectangular area covered by this node\n",
    "        self.capacity = capacity  # Max objects before subdivision\n",
    "        self.objects = []  # Objects stored in this node\n",
    "        self.children = [None, None, None, None]  # Four quadrants\n",
    "        self.is_leaf = True  # Whether this is a leaf node\n",
    "        self.neighbors = []  # Store neighboring leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Quadtree Construction ###\n",
    "def build_quad_tree(node, objects, depth=0, max_depth=20):\n",
    "    \"\"\"Recursively builds a Quadtree with improved debugging and verification.\"\"\"\n",
    "    node.objects = objects  # Store objects in node before splitting\n",
    "\n",
    "    # Base Case 1: Stop if max depth reached or too few objects\n",
    "    if depth >= max_depth or len(objects) <= node.capacity:\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Base Case 2: Stop if region size is too small\n",
    "    x_min, y_min, x_max, y_max = node.boundary.x_min, node.boundary.y_min, node.boundary.x_max, node.boundary.y_max\n",
    "    if abs(x_max - x_min) < 1e-6 or abs(y_max - y_min) < 1e-6:\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Create Quadrants\n",
    "    x_mid, y_mid = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "    quadrants = [\n",
    "        Rectangle(x_min, y_mid, x_mid, y_max),  # Top-left\n",
    "        Rectangle(x_mid, y_mid, x_max, y_max),  # Top-right\n",
    "        Rectangle(x_min, y_min, x_mid, y_mid),  # Bottom-left\n",
    "        Rectangle(x_mid, y_min, x_max, y_mid)   # Bottom-right\n",
    "    ]\n",
    "\n",
    "    node.children = [QuadTree(q, node.capacity) for q in quadrants]\n",
    "\n",
    "    # Assign Objects to Children\n",
    "    child_objects = {i: [] for i in range(4)}\n",
    "    unassigned_objects = []  # Track objects that do not fit\n",
    "\n",
    "    for obj in objects:\n",
    "        assigned = False\n",
    "        for i, child in enumerate(node.children):\n",
    "            if child.boundary.contains(obj.x, obj.y):\n",
    "                child_objects[i].append(obj)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            unassigned_objects.append(obj)\n",
    "\n",
    "    # Handle Unassigned Objects\n",
    "    if unassigned_objects:\n",
    "        node.objects = unassigned_objects  # Keep unassigned objects in this node\n",
    "        node.is_leaf = True\n",
    "        return\n",
    "\n",
    "    # Recursively Build Children\n",
    "    for i, child in enumerate(node.children):\n",
    "        if child_objects[i]: \n",
    "            build_quad_tree(child, child_objects[i], depth + 1, max_depth)\n",
    "\n",
    "    node.objects = []\n",
    "\n",
    "### Leaf Collection & Neighbor Assignment ###\n",
    "def collect_leaf_nodes(node, leaf_list=None):\n",
    "    \"\"\"Recursively collects all leaf nodes in the Quadtree.\"\"\"\n",
    "    if leaf_list is None:\n",
    "        leaf_list = []\n",
    "    if node.is_leaf:\n",
    "        leaf_list.append(node)\n",
    "    else:\n",
    "        for child in node.children:\n",
    "            if child is not None:\n",
    "                collect_leaf_nodes(child, leaf_list)\n",
    "    return leaf_list\n",
    "\n",
    "def find_neighbors_fixed(leaf_nodes):\n",
    "    \"\"\"Assigns neighbors to each leaf node based on shared boundaries.\"\"\"\n",
    "    for node in leaf_nodes:\n",
    "        node.neighbors = []\n",
    "        for other in leaf_nodes:\n",
    "            if node == other:\n",
    "                continue\n",
    "            shared_x = (node.boundary.x_max == other.boundary.x_min) or (node.boundary.x_min == other.boundary.x_max)\n",
    "            shared_y = (node.boundary.y_max == other.boundary.y_min) or (node.boundary.y_min == other.boundary.y_max)\n",
    "            if shared_x or shared_y:\n",
    "                node.neighbors.append(other)\n",
    "\n",
    "### Save Location Table ###\n",
    "def save_location_table(root, base_path, folder, batch_size=50):\n",
    "    \"\"\"Saves the location table for each leaf node using batch processing.\"\"\"\n",
    "    leaf_nodes = collect_leaf_nodes(root)\n",
    "    for i in tqdm(range(0, len(leaf_nodes), batch_size), desc=\"Saving Location Tables\", unit=\"batch\"):\n",
    "        batch = leaf_nodes[i:i+batch_size]\n",
    "        for node in batch:\n",
    "            if node.is_leaf:\n",
    "                location_table = {obj.id: (obj.x, obj.y) for obj in node.objects}\n",
    "                filename = os.path.join(base_path, f\"{folder}_location_table_{id(node)}.pkl\")\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    pickle.dump(location_table, f)\n",
    "                node.ltp = filename  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:43.398361Z",
     "iopub.status.busy": "2025-03-12T16:55:43.398058Z",
     "iopub.status.idle": "2025-03-12T16:55:50.303438Z",
     "shell.execute_reply": "2025-03-12T16:55:50.302469Z",
     "shell.execute_reply.started": "2025-03-12T16:55:43.398336Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 768.61batch/s]\n",
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 1406.54batch/s]\n",
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 641.43batch/s]\n",
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 390.20batch/s]\n",
      "Saving Location Tables: 100%|██████████| 1/1 [00:00<00:00, 274.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pass 1 complete: Quadtree built.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "quadtree_times = np.zeros(5)\n",
    "\n",
    "folders = glob.glob(os.path.join(data_path, \"tweet*\"))\n",
    "\n",
    "FOLDERS = [2, 4, 6, 8, 10]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "for folder_index in range(len(folders)):\n",
    "    objects_pass1 = []\n",
    "    files = glob.glob(os.path.join(data_path, folders[folder_index], \"*.txt\"))\n",
    "    \n",
    "    for filename in files:\n",
    "        try:\n",
    "            with open(filename, \"r\") as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) > 4:\n",
    "                        oid, x, y = parts[0], float(parts[1]), float(parts[2])\n",
    "                        objects_pass1.append(ObjectPass1(oid, x, y))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Error: File not found: {filename}\")\n",
    "            exit(1)\n",
    "\n",
    "    ### Create Quadtree ###\n",
    "    min_x, max_x = min(obj.x for obj in objects_pass1), max(obj.x for obj in objects_pass1)\n",
    "    min_y, max_y = min(obj.y for obj in objects_pass1), max(obj.y for obj in objects_pass1)\n",
    "    root_boundary = Rectangle(min_x, min_y, max_x, max_y)\n",
    "    root = QuadTree(root_boundary, capacity=4)\n",
    "\n",
    "    ### Build Quadtree ###\n",
    "    build_quad_tree(root, objects_pass1, depth=0, max_depth=25)\n",
    "\n",
    "    ### Assign Neighbors ###\n",
    "    leaf_nodes = collect_leaf_nodes(root)\n",
    "    find_neighbors_fixed(leaf_nodes)\n",
    "\n",
    "    ### Save Location Tables ###\n",
    "    save_location_table(root, data_path, FOLDERS[folder_index], batch_size=50)\n",
    "\n",
    "    ### Save Quadtree ###\n",
    "    with open(os.path.join(data_path, f\"{FOLDERS[folder_index]}_quadtree.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(root, f)\n",
    "\n",
    "    quadtree_times[folder_index] = (datetime.now() - start).total_seconds()\n",
    "\n",
    "### Final Debugging Check ###\n",
    "leaf_nodes = collect_leaf_nodes(root)\n",
    "print(f\"✅ Pass 1 complete: Quadtree built.\")\n",
    "\n",
    "np.save(\"quadtree_times.npy\", quadtree_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectPass2:\n",
    "    def __init__(self, obj_id, x, y, keywords, weights):\n",
    "        self.id = obj_id\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.keywords = keywords\n",
    "        self.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Hash Function for Consistent Node Identification\n",
    "def get_node_hash(node):\n",
    "    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n",
    "\n",
    "# Locate the Correct Leaf Node\n",
    "def locate_leaf(root, x, y):\n",
    "    node = root\n",
    "    while not node.is_leaf:\n",
    "        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n",
    "        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n",
    "\n",
    "        if y >= y_mid:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[0]  # Top Left\n",
    "            else:\n",
    "                node = node.children[1]  # Top Right\n",
    "        else:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[2]  # Bottom Left\n",
    "            else:\n",
    "                node = node.children[3]  # Bottom Right\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Build and Save Textual Indexes Using JSON\n",
    "def build_textual_indexes_as_json(root, base_path, folder):\n",
    "    leaf_nodes = collect_leaf_nodes(root)\n",
    "    json_data = {}\n",
    "\n",
    "    print(f\"✅ Processing {len(leaf_nodes)} leaf nodes for textual index...\")\n",
    "\n",
    "    for node in tqdm(leaf_nodes, desc=\"Building Textual Index\", unit=\"node\"):\n",
    "        node_hash = get_node_hash(node)\n",
    "\n",
    "        # If node is empty, ensure it is indexed with empty structures\n",
    "        if not node.objects:\n",
    "            json_data[node_hash] = {\"oti\": {}, \"iti\": {}}\n",
    "            node.iti = os.path.join(base_path, \"textual_index.json.gz\")\n",
    "            continue\n",
    "\n",
    "        # 1) Build Object Text Index (OTI)\n",
    "        text_data = {obj.id: list(zip(obj.keywords, obj.weights)) for obj in node.objects}\n",
    "\n",
    "        # 2) Build Inverted Textual Index (ITI)\n",
    "        inverted_dict = {}\n",
    "        for obj in node.objects:\n",
    "            for kw, w_str in zip(obj.keywords, obj.weights):\n",
    "                w_val = float(w_str)\n",
    "                if kw not in inverted_dict:\n",
    "                    inverted_dict[kw] = []\n",
    "                inverted_dict[kw].append((obj.id, w_val))\n",
    "\n",
    "        json_data[node_hash] = {\"oti\": text_data, \"iti\": inverted_dict}\n",
    "        node.iti = os.path.join(base_path, f\"{folder}_textual_index.json.gz\")\n",
    "\n",
    "    #Save entire JSON at once with Gzip compression\n",
    "    json_filename = os.path.join(base_path, f\"{folder}_textual_index.json.gz\")\n",
    "    with gzip.open(json_filename, \"wt\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f)\n",
    "\n",
    "    print(f\"✅ Saved all textual indexes in: {json_filename} (Compressed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:55:50.304957Z",
     "iopub.status.busy": "2025-03-12T16:55:50.304692Z",
     "iopub.status.idle": "2025-03-12T16:55:58.767518Z",
     "shell.execute_reply": "2025-03-12T16:55:58.766535Z",
     "shell.execute_reply.started": "2025-03-12T16:55:50.304933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quadtree loaded.\n",
      "✅ Loaded 100000 objects into Quadtree for Pass 2.\n",
      "✅ Processing 1 leaf nodes for textual index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Textual Index: 100%|██████████| 1/1 [00:03<00:00,  3.03s/node]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all textual indexes in: /Users/shahdivyank/Desktop/cs225-final-project/2_textual_index.json.gz (Compressed)\n",
      "✅ Quadtree loaded.\n",
      "✅ Loaded 100000 objects into Quadtree for Pass 2.\n",
      "✅ Processing 1 leaf nodes for textual index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Textual Index: 100%|██████████| 1/1 [00:12<00:00, 12.20s/node]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all textual indexes in: /Users/shahdivyank/Desktop/cs225-final-project/4_textual_index.json.gz (Compressed)\n",
      "✅ Quadtree loaded.\n",
      "✅ Loaded 100000 objects into Quadtree for Pass 2.\n",
      "✅ Processing 1 leaf nodes for textual index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Textual Index: 100%|██████████| 1/1 [00:25<00:00, 25.65s/node]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all textual indexes in: /Users/shahdivyank/Desktop/cs225-final-project/6_textual_index.json.gz (Compressed)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "pass_two_times = np.zeros(5)\n",
    "\n",
    "FOLDERS = [2, 4, 6, 8, 10]\n",
    "folders = glob.glob(os.path.join(data_path, \"tweet*\"))\n",
    "\n",
    "for folder_index in range(len(FOLDERS)):\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    # Load Quadtree\n",
    "    quadtree_path = os.path.join(data_path, f\"{FOLDERS[folder_index]}_quadtree.pkl\")\n",
    "    if not os.path.exists(quadtree_path):\n",
    "        raise FileNotFoundError(f\"❌ Error: Quadtree file not found at {quadtree_path}. Run Pass 1 first.\")\n",
    "\n",
    "    with open(quadtree_path, \"rb\") as f:\n",
    "        root = pickle.load(f)\n",
    "    print(\"✅ Quadtree loaded.\")\n",
    "\n",
    "    #Step 1: Reset objects in all leaves before inserting new ones\n",
    "    for leaf in collect_leaf_nodes(root):\n",
    "        leaf.objects = []  # Clear previous objects\n",
    "\n",
    "    # Step 2: Read and Process the Dataset Again (Pass 2)\n",
    "    filename = os.path.join(data_path, \"tweet2000000\", \"0.txt\")\n",
    "    files = glob.glob(os.path.join(data_path, folders[folder_index], \"*.txt\"))\n",
    "\n",
    "    for file in files:\n",
    "        objects_loaded = 0  # Debug counter\n",
    "\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 4:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                oid = parts[0]\n",
    "                try:\n",
    "                    x = float(parts[1])\n",
    "                    y = float(parts[2])\n",
    "                    keyword_count = int(parts[3])\n",
    "                except ValueError:\n",
    "                    print(f\"Error: Invalid number format in line: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                #Read (keyword, weight) pairs\n",
    "                keywords = []\n",
    "                weights = []\n",
    "                idx = 4\n",
    "                for _ in range(keyword_count):\n",
    "                    try:\n",
    "                        kw = parts[idx]\n",
    "                        wt = float(parts[idx + 1])  # Convert weight to float\n",
    "                        keywords.append(kw)\n",
    "                        weights.append(wt)\n",
    "                        idx += 2\n",
    "                    except (IndexError, ValueError):\n",
    "                        print(f\"Skipping malformed keyword-weight pair in line: {line.strip()}\")\n",
    "                        break\n",
    "\n",
    "                # Create an Object for Pass 2\n",
    "                obj2 = ObjectPass2(oid, x, y, keywords, weights)\n",
    "\n",
    "                # Insert into the correct leaf node\n",
    "                leaf_node = locate_leaf(root, x, y)\n",
    "                leaf_node.objects.append(obj2)\n",
    "                objects_loaded += 1  # Increment debug counter\n",
    "    \n",
    "    pass_two_times[folder_index] = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    print(f\"✅ Loaded {objects_loaded} objects into Quadtree for Pass 2.\")\n",
    "\n",
    "    #Step 3: Build textual indexes using JSON storage\n",
    "    build_textual_indexes_as_json(root, data_path, FOLDERS[folder_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded textual index from: /Users/shahdivyank/Desktop/cs225-final-project/6_textual_index.json.gz\n",
      "✅ Example Node Hashes stored in textual index: ['e5c55af39ff83355']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "# Load Textual Index\n",
    "def load_textual_indexes(base_path):\n",
    "    \"\"\"Loads the JSON-based textual index from a compressed file.\"\"\"\n",
    "    json_filename = os.path.join(base_path, \"6_textual_index.json.gz\")\n",
    "\n",
    "    if not os.path.exists(json_filename):\n",
    "        raise FileNotFoundError(f\"❌ Error: Textual index file not found at {json_filename}. Run Pass 2 to generate this file.\")\n",
    "\n",
    "    with gzip.open(json_filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "        textual_index = json.load(f)\n",
    "\n",
    "    print(f\"✅ Loaded textual index from: {json_filename}\")\n",
    "    return textual_index\n",
    "\n",
    "# Load the textual index\n",
    "textual_index = load_textual_indexes(data_path)\n",
    "\n",
    "# Print a few stored node hashes\n",
    "stored_node_hashes = list(textual_index.keys())  # Print first 10 hashes\n",
    "print(\"✅ Example Node Hashes stored in textual index:\", stored_node_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import pickle\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Query Class\n",
    "class Query:\n",
    "    \"\"\"\n",
    "    Represents a spatial-textual query.\n",
    "    \"\"\"\n",
    "    def __init__(self, loc, pos_keywords=None, neg_phrases=None, k=10, lam=0.5):\n",
    "        self.loc = loc\n",
    "        self.pos = pos_keywords if pos_keywords else []\n",
    "        self.neg = neg_phrases if neg_phrases else []\n",
    "        self.k = k\n",
    "        self.lam = lam  # Lambda controls weighting between spatial & textual scores\n",
    "\n",
    "# Hash Function for Node Identification\n",
    "def get_node_hash(node):\n",
    "    \"\"\"Generates a unique, persistent identifier for a Quadtree node based on its boundary.\"\"\"\n",
    "    key = f\"{node.boundary.x_min}_{node.boundary.y_min}_{node.boundary.x_max}_{node.boundary.y_max}\"\n",
    "    return hashlib.md5(key.encode()).hexdigest()[:16]  # Shortened for efficiency\n",
    "\n",
    "# Distance Calculation\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Computes Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n",
    "\n",
    "def normalized_spatial_score(obj_loc, query_loc, max_dist=1000.0):\n",
    "    \"\"\"Computes a normalized spatial score (1 = closest, 0 = farthest).\"\"\"\n",
    "    dist = euclidean_distance(obj_loc, query_loc)\n",
    "    return 1.0 - min(dist / max_dist, 1.0)\n",
    "\n",
    "# Negative Keyword Filtering\n",
    "def violates_negative_phrases(obj_id, iti, neg_phrases):\n",
    "    \"\"\"\n",
    "    Checks if the object contains any negative phrase match.\n",
    "    Fix: Uses phrase-based filtering instead of individual word filtering.\n",
    "    \"\"\"\n",
    "    if not neg_phrases:\n",
    "        return False\n",
    "\n",
    "    for phrase in neg_phrases:\n",
    "        words = phrase.split()\n",
    "        if all(word in iti and obj_id in [x[0] for x in iti[word]] for word in words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Local POWER Algorithm\n",
    "def local_POWER(node, q, textual_index):\n",
    "    \"\"\"\n",
    "    Performs local POWER query processing on a given node.\n",
    "    Uses the stored textual indexes to rank objects.\n",
    "    \"\"\"\n",
    "    node_hash = get_node_hash(node)\n",
    "\n",
    "    if node_hash not in textual_index:\n",
    "        return []\n",
    "\n",
    "    oti = textual_index[node_hash][\"oti\"]\n",
    "    iti = textual_index[node_hash][\"iti\"]\n",
    "\n",
    "    # Step 1: Build a spatially sorted list\n",
    "    spatial_list = []\n",
    "    for obj_id in oti.keys():\n",
    "        obj_loc = node.boundary.x_min, node.boundary.y_min  # Approximate object location\n",
    "        sp_score = normalized_spatial_score(obj_loc, q.loc)\n",
    "        spatial_list.append((obj_id, sp_score))\n",
    "\n",
    "    spatial_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Step 2: Textual scoring\n",
    "    def compute_textual_score(obj_id):\n",
    "        score = 0.0\n",
    "        for kw in q.pos:\n",
    "            if kw in iti and obj_id in [x[0] for x in iti[kw]]:\n",
    "                score += 1.0\n",
    "        return min(score, 1.0)\n",
    "\n",
    "    # Step 3: Compute final ranking\n",
    "    local_top_k = []\n",
    "    for obj_id, sp_score in spatial_list:\n",
    "        txt_score = compute_textual_score(obj_id)\n",
    "        if violates_negative_phrases(obj_id, iti, q.neg):\n",
    "            continue\n",
    "        final_score = q.lam * sp_score + (1 - q.lam) * txt_score\n",
    "        heapq.heappush(local_top_k, (-final_score, obj_id))\n",
    "\n",
    "    return heapq.nsmallest(q.k, local_top_k)\n",
    "\n",
    "# Locate the Correct Leaf Node\n",
    "def locate_leaf(root, x, y):\n",
    "    \"\"\"Finds the correct leaf node where an object (x, y) belongs.\"\"\"\n",
    "    node = root\n",
    "    while not node.is_leaf:\n",
    "        x_mid = (node.boundary.x_min + node.boundary.x_max) / 2\n",
    "        y_mid = (node.boundary.y_min + node.boundary.y_max) / 2\n",
    "\n",
    "        if y >= y_mid:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[0]\n",
    "            else:\n",
    "                node = node.children[1]\n",
    "        else:\n",
    "            if x <= x_mid:\n",
    "                node = node.children[2]\n",
    "            else:\n",
    "                node = node.children[3]\n",
    "    return node\n",
    "\n",
    "# **Batch Query Processing**\n",
    "def batch_process_power(root, queries, textual_index):\n",
    "    \"\"\"\n",
    "    Implements batch query processing for spatial-textual queries.\n",
    "    Optimized for shared node access.\n",
    "    \"\"\"\n",
    "    query_groups = defaultdict(list)\n",
    "\n",
    "    # Step 1: Group queries by leaf node\n",
    "    for query in queries:\n",
    "        leaf = locate_leaf(root, query.loc[0], query.loc[1])\n",
    "        query_groups[leaf].append(query)\n",
    "\n",
    "    batch_results = {}\n",
    "\n",
    "    # Step 2: Process queries in batches\n",
    "    for leaf, grouped_queries in query_groups.items():\n",
    "        node_hash = get_node_hash(leaf)\n",
    "\n",
    "        if node_hash not in textual_index:\n",
    "            continue\n",
    "\n",
    "        results_per_query = {}\n",
    "        for q in grouped_queries:\n",
    "            results_per_query[q] = local_POWER(leaf, q, textual_index[node_hash])\n",
    "\n",
    "        batch_results[node_hash] = results_per_query\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "# Load Quadtree and Textual Index\n",
    "with open(os.path.join(data_path, \"6_quadtree.pkl\"), \"rb\") as f:\n",
    "    root = pickle.load(f)\n",
    "print(\"Quadtree loaded.\")\n",
    "\n",
    "textual_index = load_textual_indexes(data_path)\n",
    "\n",
    "# Define Sample Queries\n",
    "queries = [\n",
    "    Query(loc=(-21.18474656, -47.78603054), pos_keywords=[\"pizza\"], k=5, lam=0.8),\n",
    "    # Query(loc=(700.0, 300.0), pos_keywords=[\"burger\"], neg_phrases=[], k=3, lam=0.5),\n",
    "    # Query(loc=(200.0, 800.0), pos_keywords=[\"sushi\"], neg_phrases=[\"bad\"], k=7, lam=0.6),\n",
    "]\n",
    "\n",
    "# Run Batch Query Processing\n",
    "batch_results = batch_process_power(root, queries, textual_index)\n",
    "\n",
    "# Display Results\n",
    "for node_hash, query_results in batch_results.items():\n",
    "    print(f\"Node Hash: {node_hash}\")\n",
    "    for query, results in query_results.items():\n",
    "        print(f\" Query at {query.loc} => {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the Boolean range query\n",
    "\n",
    "#First find the leaf nodes within that range(provided xmax,xmin,ymax,ymin)\n",
    "\n",
    "#Separately using the Positive keywords and negative keywords to find the corresponding object ids for each keyword and for each group\n",
    "\n",
    "#From positive keyword group, we remove the overlapping obeject ids in the negative group\n",
    "\n",
    "#output the remaining object ids as our query output\n",
    "        \n",
    "def boolean_range_query(root, xmin, xmax, ymin, ymax, pos_keywords, neg_keywords, textual_index, boolean = \"OR\"):\n",
    "    \"\"\"\n",
    "    Implements a Boolean range query.\n",
    "    \n",
    "    Steps:\n",
    "    1. Identify leaf nodes within the given spatial range.\n",
    "    2. Retrieve object IDs separately for positive and negative keyword groups.\n",
    "    3. Remove overlapping object IDs from the negative keyword group.\n",
    "    4. Return the filtered object IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    - root: The root of the Quadtree.\n",
    "    - xmin, xmax, ymin, ymax: The bounding box defining the query range.\n",
    "    - pos_keywords: List of positive keywords.\n",
    "    - neg_keywords: List of negative keywords.\n",
    "    - textual_index: The inverted textual index (ITI).\n",
    "\n",
    "    Returns:\n",
    "    - A set of object IDs that match the positive keywords but exclude negative ones.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Find all leaf nodes within the range\n",
    "    def get_leaf_nodes_in_range(node, xmin, xmax, ymin, ymax, result = None):\n",
    "        \"\"\"Recursively collect leaf nodes that intersect with the range.\"\"\"\n",
    "        if result is None:\n",
    "            result = []\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            if not (node.boundary.x_max < xmin or node.boundary.x_min > xmax or \n",
    "                    node.boundary.y_max < ymin or node.boundary.y_min > ymax):\n",
    "                result.append(node)\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                if child is not None:\n",
    "                    get_leaf_nodes_in_range(child, xmin, xmax, ymin, ymax, result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    leaf_nodes = get_leaf_nodes_in_range(root, xmin, xmax, ymin, ymax)\n",
    "\n",
    "    # Step 2: Collect object IDs based on positive and negative keyword groups\n",
    "    pos_object_ids = set()\n",
    "    neg_object_ids = set()\n",
    "\n",
    "    for node in leaf_nodes:\n",
    "        node_hash = get_node_hash(node)\n",
    "        if node_hash not in textual_index:\n",
    "            continue\n",
    "        \n",
    "        iti = textual_index[node_hash][\"iti\"]\n",
    "\n",
    "        # Collect object IDs for positive keywords        \n",
    "        for kw in pos_keywords:\n",
    "            if kw in iti:\n",
    "                pos_object_ids.update(obj_id for obj_id, _ in iti[kw])\n",
    "\n",
    "        # Collect object IDs for negative keywords\n",
    "        for kw in neg_keywords:\n",
    "            if kw in iti:\n",
    "                neg_object_ids.update(obj_id for obj_id, _ in iti[kw])\n",
    "\n",
    "    # Step 3: Remove negative keyword matches from positive object IDs\n",
    "    filtered_object_ids = pos_object_ids - neg_object_ids\n",
    "\n",
    "    return filtered_object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define spatial range (bounding box)\n",
    "positive_words_times = np.zeros(5)\n",
    "negative_words_times = np.zeros(5)\n",
    "positive_negative_times = np.zeros(5)\n",
    "\n",
    "xmin, xmax = -50.0, 50.0  # X-coordinate range\n",
    "ymin, ymax = -50.0, 50.0  # Y-coordinate range\n",
    "\n",
    "positive_words = [\"food\", \"pizza\", \"chicago\", \"photos\", \"drink\"]\n",
    "negative_words = [\"bad\", \"slow\", \"traffic\", \"dirty\", \"scared\"]\n",
    "\n",
    "for i in range(5):\n",
    "    start = datetime.now()\n",
    "    filtered_results = boolean_range_query(root, xmin, xmax, ymin, ymax, positive_words[:i], [], textual_index)\n",
    "    positive_words_times[i] = (datetime.now() - start).total_seconds()\n",
    "\n",
    "for i in range(5):\n",
    "    start = datetime.now()\n",
    "    filtered_results = boolean_range_query(root, xmin, xmax, ymin, ymax, [], negative_words[:i], textual_index)\n",
    "    negative_words_times[i] = (datetime.now() - start).total_seconds()\n",
    "\n",
    "for i in range(5):\n",
    "    start = datetime.now()\n",
    "    filtered_results = boolean_range_query(root, xmin, xmax, ymin, ymax, positive_words[:i], negative_words[:i], textual_index)\n",
    "    positive_negative_times[i] = (datetime.now() - start).total_seconds()\n",
    "\n",
    "np.save(\"6_positive_words_times.npy\", positive_words_times)\n",
    "np.save(\"6_negative_words_times.npy\", negative_words_times)\n",
    "np.save(\"6_positive_negative_times.npy\", positive_negative_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1eCnXs3JC7d1b5nxaiCiOzE-F4bQt4x-v",
     "timestamp": 1741586207357
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6851434,
     "sourceId": 11005628,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
